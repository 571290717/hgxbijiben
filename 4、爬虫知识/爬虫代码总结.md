# 爬虫代码总结（自己敲）

```python
#爬虫：模拟浏览器，发送请求，获取响应


流程： 
1、获取一个url
2、向url发送请求、并获取响应（需要http协议）
3、如果从响应中提取url，则继续发送请求获取响应
4、如果从响应中提取数据，则将数据进行保存



HTTP:超文本传输协议，默认端口号80 
    超文本：是指超过文本，不仅限于文本;还包括图片、音频、视频等文件
    传输协议：是指使用共用待定的固定格式来传递转换成字符串的超文本内容
HTTPS：HTTP+SSl(安全套接字层)，即带有安全套接字层的超文本传输协议，默认端口号443
	SSL对传输的内容(超文本，也就是请求体或者响应体)进步加密
  
content—type 文本类型
Host
Connection
Upgrade-Insecure-Requests  |HTTPS
user-Agent
Referer
Cookie
Authorization


200 
302
303
307
403
404
500
503

http请求的过程：
	1、浏览器在拿到域名对应的ip后，先向地址栏中的url发送请求，并获取响应
    2、在返回的响应内容（html）中，会带有css、js、图片等url地址，以及ajax代码，浏览器按照响应内容中的顺序依次发送其他的请求，并获取相应的响应
    3、浏览器每获取一个响应就对展出的结果进行添加（加载），js，css等内容会修改页面的内容，js也可以重新发送请求，获取响应
    4、从获取第一个响应并在浏览器，中展示，直到最终获取全部响应，并在展示的结果中添加内容或修改————渲染
    
##### 浏览器展示的结果可以由多次请求对应的多次响应共同渲染出来，而爬虫是一次请求对应一个响应


request模块
发送http请求，获取响应

pip install requests

import requests

url = "xxxx "
response = requests.get(url)
print(response.text)

print(response.content.decode())
response.text = response.content.decode('推测出的编码字符集')

response.url
response.status_code
response.request.headers
response.headers
response.request._cookies
response.cookies
response.json()

# 打印响应内容
# print(response.text)
# print(response.content.decode()) 			# 注意这里！
print(response.url)							# 打印响应的url
print(response.status_code)					# 打印响应的状态码
print(response.request.headers)				# 打印响应对象的请求头
print(response.headers)						# 打印响应头
print(response.request._cookies)			# 打印请求携带的cookies
print(response.cookies)						# 打印响应中携带的cookies



import resquests
url='https://www.baidu,com'
header={ "User-Agent": "xxxx",
       	"Cookie":'xxxcookie'
       }
kw={'xx1':'xx2'}
response = resquest.get(url,headers = headers,params=kw)
print(response.content)
print(response.request.headers)

cookies = {"cookies_name":"cookies_value"}

cookies_str = "复制的cookiesxxxxx"
cookies_dict = {cookie.split('=')[0]:cookie.split('=')[-1] for cookie in cookies_str.split("; ")}
resp = requests.get(url,headers=headers,cookies=cookies_dict)


cookies_dict = resquest.utils.dict_form_cookiejar(response.cookies)
==> 返回cookies字典


response = resquests.get(url.timeout=3)

代理
Transparent Proxy
Anonymous Proxy
Elite proxy or High Anonymity Proxy

proxies = {
    "http":"xxxxxxx",
    "https": "xxxxxx",
}

response = request.get(url,proxies=proxies)#!!!!!!!!



##项目百度翻译
import requests
import json



class King(object):
    
    def __init__(self,word):
        self.url = "http://fy.iciba.com/ajax.php?a=fy"
        self.word = word
        self.headers = {
            
            "User-Agent":'xxxxx'
        }
		self.post_data = {
            "f" : "auto",
            "t" : "auto",
            "w" : self.word
        }

	def get_data(self):
        response = requests.post(self.url,headers=self.headers,data=self.post_data)
        return response.content
    
    def parse_data(self,data):
        dict_data = json.loads(data)
        
        try:
            print(dict_data['content']['out'])
		except:
            print(dict_data['content']['word_mean'][0])
   
	def run(self):
        data = self.get_data()
        self.parse_data(data)
        
if __name__ == '__main__':
    king=King("china")
	king.run()
    
    
    ## 利用requests.session进行状态保持

> requests模块中的Session类能够自动处理发送请求获取响应过程中产生的cookie，进而达到状态保持的目的。接下来我们就来学习它
- requests.session的作用
  - 自动处理cookie，即 **下一次请求会带上前一次的cookie**
- requests.session的应用场景
  - 自动处理连续的多次请求过程中产生的cookie
    
    session = requests.session()
    response = session.get(url,header,...)
    response = session.post(url,data,...)
    
    
import requests
import re


# 构造请求头字典
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36',
}

# 实例化session对象
session = requests.session()

# 访问登陆页获取登陆请求所需参数
response = session.get('https://github.com/login', headers=headers)
authenticity_token = re.search('name="authenticity_token" value="(.*?)" />', response.text).group(1) # 使用正则获取登陆请求所需参数

# 构造登陆请求参数字典
data = {
    'commit': 'Sign in', # 固定值
    'utf8': '✓', # 固定值
    'authenticity_token': authenticity_token, # 该参数在登陆页的响应内容中
    'login': input('输入github账号：'),
    'password': input('输入github账号：')
}

# 发送登陆请求（无需关注本次请求的响应）
session.post('https://github.com/session', headers=headers, data=data)

# 打印需要登陆后才能访问的页面
response = session.get('https://github.com/1596930226', headers=headers)
print(response.text)



#######数据提取概要#####


- html：
  - 超文本标记语言
  - 为了更好的显示数据，侧重点是为了显示
- xml：
  - 可扩展标记语言
  - 为了传输和存储数据，侧重点是在于数据内容本身


#jsonpath
pip install jsonpath

from jsonpath import jsonpath
 ret = jsonpath(a,'jsonpath语法规则字符串')
     
 
#jsonpath语法规则
 $  
 @
 . or[]
 n/a
..
*
n/a
[]
[,]
?()
()
n/a


eg:
    $.store.book[*].author
    $..author
    $.store.*
    $.store..price
    $..book[2]
    $..book[(@.length-1)] | $..book[-1:]
    $..book[0,1] | $..book[:2]
    $..book[?(@.isbn)]
    $..book[?(@.price<10)]
    $..*
##拉勾网

import requests
import jsonpath
import json

url = "xxxxx"
headers = {"User-Agent": "xxxxx"}
response = request.get(url,header=headers)
html_str = response.content.decode()

jsonobj = json.loads(html_str)
citylist = jsonpath.jsonpath(jsonobj,"$..name")

with open('city_name.txt','w') as f:
    content = json.dumps(citylist,ensure_ascii=False)
    f.write(content)


###数据提取-lxml模块

XPATH （参考selenium）
pip install lxml

html = etree.HTML(text)
ret_list=html.xpath('XPATH语法规则')






























































```

![image-20220926155515004](C:\Users\GREE\AppData\Roaming\Typora\typora-user-images\image-20220926155515004.png)

![image-20220926162547152](C:\Users\GREE\AppData\Roaming\Typora\typora-user-images\image-20220926162547152.png)

![image-20220926162559277](C:\Users\GREE\AppData\Roaming\Typora\typora-user-images\image-20220926162559277.png)
