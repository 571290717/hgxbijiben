# 爬虫代码总结（自己敲）

````python
#爬虫：模拟浏览器，发送请求，获取响应


流程： 
1、获取一个url
2、向url发送请求、并获取响应（需要http协议）
3、如果从响应中提取url，则继续发送请求获取响应
4、如果从响应中提取数据，则将数据进行保存



HTTP:超文本传输协议，默认端口号80 
    超文本：是指超过文本，不仅限于文本;还包括图片、音频、视频等文件
    传输协议：是指使用共用待定的固定格式来传递转换成字符串的超文本内容
HTTPS：HTTP+SSl(安全套接字层)，即带有安全套接字层的超文本传输协议，默认端口号443
	SSL对传输的内容(超文本，也就是请求体或者响应体)进步加密
  
content—type 文本类型
Host
Connection
Upgrade-Insecure-Requests  |HTTPS
user-Agent
Referer
Cookie
Authorization


200 
302
303
307
403
404
500
503

http请求的过程：
	1、浏览器在拿到域名对应的ip后，先向地址栏中的url发送请求，并获取响应
    2、在返回的响应内容（html）中，会带有css、js、图片等url地址，以及ajax代码，浏览器按照响应内容中的顺序依次发送其他的请求，并获取相应的响应
    3、浏览器每获取一个响应就对展出的结果进行添加（加载），js，css等内容会修改页面的内容，js也可以重新发送请求，获取响应
    4、从获取第一个响应并在浏览器，中展示，直到最终获取全部响应，并在展示的结果中添加内容或修改————渲染
    
##### 浏览器展示的结果可以由多次请求对应的多次响应共同渲染出来，而爬虫是一次请求对应一个响应


request模块
发送http请求，获取响应

pip install requests

import requests

url = "xxxx "
response = requests.get(url)
print(response.text)

print(response.content.decode())
response.text = response.content.decode('推测出的编码字符集')

response.url
response.status_code
response.request.headers
response.headers
response.request._cookies
response.cookies
response.json()

# 打印响应内容
# print(response.text)
# print(response.content.decode()) 			# 注意这里！
print(response.url)							# 打印响应的url
print(response.status_code)					# 打印响应的状态码
print(response.request.headers)				# 打印响应对象的请求头
print(response.headers)						# 打印响应头
print(response.request._cookies)			# 打印请求携带的cookies
print(response.cookies)						# 打印响应中携带的cookies



import resquests
url='https://www.baidu,com'
header={ "User-Agent": "xxxx",
       	"Cookie":'xxxcookie'
       }
kw={'xx1':'xx2'}
response = resquest.get(url,headers = headers,params=kw)
print(response.content)
print(response.request.headers)

cookies = {"cookies_name":"cookies_value"}

cookies_str = "复制的cookiesxxxxx"
cookies_dict = {cookie.split('=')[0]:cookie.split('=')[-1] for cookie in cookies_str.split("; ")}
resp = requests.get(url,headers=headers,cookies=cookies_dict)


cookies_dict = resquest.utils.dict_form_cookiejar(response.cookies)
==> 返回cookies字典


response = resquests.get(url.timeout=3)

代理
Transparent Proxy
Anonymous Proxy
Elite proxy or High Anonymity Proxy

proxies = {
    "http":"xxxxxxx",
    "https": "xxxxxx",
}

response = request.get(url,proxies=proxies)#!!!!!!!!



##项目百度翻译
import requests
import json



class King(object):
    
    def __init__(self,word):
        self.url = "http://fy.iciba.com/ajax.php?a=fy"
        self.word = word
        self.headers = {
            
            "User-Agent":'xxxxx'
        }
		self.post_data = {
            "f" : "auto",
            "t" : "auto",
            "w" : self.word
        }

	def get_data(self):
        response = requests.post(self.url,headers=self.headers,data=self.post_data)
        return response.content
    
    def parse_data(self,data):
        dict_data = json.loads(data)
        
        try:
            print(dict_data['content']['out'])
		except:
            print(dict_data['content']['word_mean'][0])
   
	def run(self):
        data = self.get_data()
        self.parse_data(data)
        
if __name__ == '__main__':
    king=King("china")
	king.run()
    
    
    ## 利用requests.session进行状态保持

> requests模块中的Session类能够自动处理发送请求获取响应过程中产生的cookie，进而达到状态保持的目的。接下来我们就来学习它
- requests.session的作用
  - 自动处理cookie，即 **下一次请求会带上前一次的cookie**
- requests.session的应用场景
  - 自动处理连续的多次请求过程中产生的cookie
    
    session = requests.session()
    response = session.get(url,header,...)
    response = session.post(url,data,...)
    
    
import requests
import re


# 构造请求头字典
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36',
}

# 实例化session对象
session = requests.session()

# 访问登陆页获取登陆请求所需参数
response = session.get('https://github.com/login', headers=headers)
authenticity_token = re.search('name="authenticity_token" value="(.*?)" />', response.text).group(1) # 使用正则获取登陆请求所需参数

# 构造登陆请求参数字典
data = {
    'commit': 'Sign in', # 固定值
    'utf8': '✓', # 固定值
    'authenticity_token': authenticity_token, # 该参数在登陆页的响应内容中
    'login': input('输入github账号：'),
    'password': input('输入github账号：')
}

# 发送登陆请求（无需关注本次请求的响应）
session.post('https://github.com/session', headers=headers, data=data)

# 打印需要登陆后才能访问的页面
response = session.get('https://github.com/1596930226', headers=headers)
print(response.text)



#######数据提取概要#####


- html：
  - 超文本标记语言
  - 为了更好的显示数据，侧重点是为了显示
- xml：
  - 可扩展标记语言
  - 为了传输和存储数据，侧重点是在于数据内容本身


#jsonpath
pip install jsonpath

from jsonpath import jsonpath
 ret = jsonpath(a,'jsonpath语法规则字符串')
     
 
#jsonpath语法规则
 $  
 @
 . or[]
 n/a
..
*
n/a
[]
[,]
?()
()
n/a


eg:
    $.store.book[*].author
    $..author
    $.store.*
    $.store..price
    $..book[2]
    $..book[(@.length-1)] | $..book[-1:]
    $..book[0,1] | $..book[:2]
    $..book[?(@.isbn)]
    $..book[?(@.price<10)]
    $..*
##拉勾网

import requests
import jsonpath
import json

url = "xxxxx"
headers = {"User-Agent": "xxxxx"}
response = request.get(url,header=headers)
html_str = response.content.decode()

jsonobj = json.loads(html_str)
citylist = jsonpath.jsonpath(jsonobj,"$..name")

with open('city_name.txt','w') as f:
    content = json.dumps(citylist,ensure_ascii=False)
    f.write(content)


###数据提取-lxml模块

XPATH （参考selenium）
pip install lxml

W3School官方文档：<http://www.w3school.com.cn/xpath/index.asp>
    
html = etree.HTML(text)
ret_list=html.xpath('XPATH语法规则')



from lxml import etree

text = ''' 
<div> 
  <ul> 
    <li class="item-1">
      <a href="link1.html">first item</a>
    </li> 
    <li class="item-1">
      <a href="link2.html">second item</a>
    </li> 
    <li class="item-inactive">
      <a href="link3.html">third item</a>
    </li> 
    <li class="item-1">
      <a href="link4.html">fourth item</a>
    </li> 
    <li class="item-0">
      a href="link5.html">fifth item</a>
  </ul> 
</div>
'''
#利用etree.HTML，将html字符串（bytes类型或str类型）转化为Element对象，Element对象具有xpath的方法，返回结果的列表
html = etree.HTML(text)

#获取href的列表和title的列表
href_list = html.xpath("//li[@class='item-1']/a/@href")
titlr_list = html.xpath("//li[@class='item-1']/a/text()")

#组装成字典
for href in href_list:
    item = {}
    item["href"] = href
    item["title"] = title_list[href_list.index(href)]
	print(item)
    
  
- lxml.etree.HTML(html_str)可以自动补全标签
- `lxml.etree.tostring`函数可以将转换为Element对象再转换回html字符串
- 爬虫如果使用lxml来提取数据，应该以`lxml.etree.tostring`的返回结果作为提取数据的依据
    



from lxml import etree

html = etree.HTML(html_str)

handeled_html_str = etree.tostring(html).decode()




###selenium


from selenium import webdriver

driver = webdriver.Chrome()
driver.get("http://www.itcast.cn/")
print(driver.title)
driver.quit()

##无界 PhantomJS 是一个基于Webkit的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的 JavaScript。下载地址：<http://phantomjs.org/download.html>
driver = webdriver.PhantomJS(executable_path = '/home/worker/Desktop/driver/phantomjs')
driver.get("http://www.itcast.cn/")
driver.save_screenshot("itcast.png")
driver.quit()



driver.page_source
driver.current_url
driver.screen_shot(img_name)


#手动实现页面等待

import time
from selenium import webdriver 
driver = webdriver.Chome("XXXX")

driver.get("XXXX")
time.sleep(1)

for i in range(10):
    i +=1
    try :
        time.sleep(3)
        element = driver .find_element_by_xpath('XXXX')
        print(element.get_attribute("href"))
        break
    except:
        js = "window.scrollTo(1,{})".format(i*500)
        driver.execute_script(js)
 
driver.quit()





 #selenium开启无界面模式

options = webdriver.ChromeOptions()
options.add_argument("--headless")
options.add_argument("--disable-gpu")
driver = webdriver.Chrome(chrome_option=options)



from selenium import webdriver

options = webdriver.ChromeOptions()
options.add_argument("--headless")
options.add_argument("--disable-gpu")

driver = webdriver.Chrome(chrome_options=options)
driver.get("http://ww.itcast.cn")
print(driver.title)
driver.quit()


## 使用代理 ip
options = webdriver.ChromeOptions()
options.add_argument('--proxy-server=http://202.20.16.82:9527')
driver = webdriver.Chrome(chrome_options=options)

#selenium替换user—agent
options=webdriver.ChromeOptions()
options.add_argument("--user-agent=Mozilla/5.0 HAHA")
driver = webdriver.Chrome("./chromedriver",chrome_options=options)

from selenium import webdriver

options = webdriver.ChromeOptions()
options.add_argument('XXXXX')

driver = webdriver.Chrome('./chromedriver',chrome_options=options)

driver.get('http://www.itcast.cn')
print(driver.title)
driver.quit()



#####反爬与反反爬


反爬的三个方向：
基于身份识别
基于爬虫行为
基于数据加密

掌握 常见的反爬手段、原理以及应对思路

文字性内容参照大总结



###图片验证码
### .图片识别引擎

> OCR（Optical Character Recognition）是指使用扫描仪或数码相机对文本资料进行扫描成图像文件，然后对图像文件进行分析处理，自动识别获取文字信息及版面信息的软件。

#### 2.1 什么是tesseract

- Tesseract，一款由HP实验室开发由Google维护的开源OCR引擎，特点是开源，免费，支持多语言，多平台。
- 项目地址：https://github.com/tesseract-ocr/tesseract   

pip install pillow
pip/pip3 install pytesseract

通过pytesseract模块的 image_to_string 方法就能将打开的图片文件中的数据提取成字符串数据

from PIL import Image
import pytesseract

im = Image.open()

result = pytessseract.image_to_string(im)

print(result)


#### 2.4 图片识别引擎的使用扩展

- [tesseract简单使用与训练](https://www.cnblogs.com/cnlian/p/5765871.html)
- 其他ocr平台

```
    微软Azure 图像识别：https://azure.microsoft.com/zh-cn/services/cognitive-services/computer-vision/
    有道智云文字识别：http://aidemo.youdao.com/ocrdemo
    阿里云图文识别：https://www.aliyun.com/product/cdi/
    腾讯OCR文字识别：https://cloud.tencent.com/product/ocr
```

### 


?################---Mongodb数据库---##############!!!!!!!!!

非关系性数据库mongodb


1. 了解 非关系型数据库的优势
   - 易扩展
   - 高性能
   - 灵活的数据字段
2. 了解 mongodb的安装
   - sudo apt-get install -y mongodb-org

- 默认端口：27017
- 默认配置文件的位置：/etc/mongod.conf
- 默认日志的位置：/var/log/mongodb/mongod.log


mongodb服务端启动分别两种方式：

- 本地测试方式的启动（只具有本地数据增删改查的功能）
- 生产环境启动（具有完整的全部功能）

#### 1.1 测试方式启动

- 启动: sudo service mongod start (sudo service mongod start)
- 停止: sudo service mongod stop
- 重启: sudo service mongod restart

#### 1.2 生产环境正式的启动方式

> 启动: sudo mongod [--auth --dbpath=dbpath --logpath=logpath --append --fork] [-–f logfile ]

- 只以 sudo mongod 命令启动时，默认将数据存放在了 /data/db 目录下，需要手动创建
- --dbpath: 指定数据库的存放路径
- --logpath: 指定日志的存放路径
- --append: 或--logappend 设置日志的写入形式为追加模式
- --fork: 或-fork 开启新的进程运行mongodb服务
- --f: 或-f 配置文件路径(可以将上述配置信息写入文件然后通过该文件中的参数进行加载启动)
- --auth: 以权限认证的方式启动，我们会在后边的课程中学习该内容

#### 1.3 查看是否启动成功

> ps aux | grep mongod

### 2. 启动mongodb的客户端：进入mongo shell

- 启动本地客户端: mongo
- 查看帮助：mongo –help
- 退出：exit或者ctrl+c

（'等自己下载一个mongodb实战后再回来写'））












````

![image-20220926155515004](C:\Users\GREE\AppData\Roaming\Typora\typora-user-images\image-20220926155515004.png)

![image-20220926162547152](C:\Users\GREE\AppData\Roaming\Typora\typora-user-images\image-20220926162547152.png)

![image-20220926162559277](C:\Users\GREE\AppData\Roaming\Typora\typora-user-images\image-20220926162559277.png)

W3School官方文档：<http://www.w3school.com.cn/xpath/index.asp>

**PhantomJS 是一个基于Webkit的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的 JavaScript。下载地址：<http://phantomjs.org/download.html>**



#### 2.1 什么是tesseract

- Tesseract，一款由HP实验室开发由Google维护的开源OCR引擎，特点是开源，免费，支持多语言，多平台。
- 项目地址：https://github.com/tesseract-ocr/tesseract   

#### 2.4 图片识别引擎的使用扩展

- [tesseract简单使用与训练](https://www.cnblogs.com/cnlian/p/5765871.html)
- 

### 3 打码平台

#### 1.为什么需要了解打码平台的使用

现在很多网站都会使用验证码来进行反爬，所以为了能够更好的获取数据，需要了解如何使用打码平台爬虫中的验证码

#### 2 常见的打码平台

1. 云打码：http://www.yundama.com/

   能够解决通用的验证码识别

2. 极验验证码智能识别辅助：http://jiyandoc.c2567.com/

   能够解决复杂验证码的识别
