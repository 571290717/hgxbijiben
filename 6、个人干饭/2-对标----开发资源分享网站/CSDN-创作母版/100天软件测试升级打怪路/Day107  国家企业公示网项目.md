# Day107  国家企业公示网项目



# 爬虫项目-国家企业公示网（可选）

## 介绍

根据企业名称抓取[国家企业信息公示网](http://www.gsxt.gov.cn/) 中的企业信息数据

网址：

- 主站url http://www.gsxt.gov.cn/ 
- 江苏地区url http://www.jsgsj.gov.cn:58888/province/

### 提示：该网站极其不稳定

## 内容

- 项目分析
- webapi：类flask框架quart
- node_server：节点内启动爬虫
- crawler：利用selenium进行数据抓取
- 运行效果截图



## 国家企业公示网项目分析

##### 学习目标：

1. 了解 获取数据的抓取流程
2. 了解 每个组件的功能
3. 了解 项目运行流程

_________________


> 演示运行效果，确定被抓取网站状态正常


### 1. 确定抓取流程，确定数据位置

> 使用selenium控制浏览器进行抓取

#### 1.1 网站首页

![网站首页](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\1.1.1.png)

#### 1.2 行为验证图片

> 在输入企业名称点击查询后有可能会跳出验证图片，我们可以采取手动打码或使用第三方打码平台的方式来获取验证图片的点击坐标

![行为验证图片](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\1.1.2.png)

#### 1.3 选择列表页中第一个公司

![公司列表页](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\1.1.3.png)

#### 1.4 确定数据位置

![数据位置](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\1.1.4.png)

#### 1.5 保存数据页面

> 数据量庞大，可以先保存数据的各个页面到本地，后续再进行提取

![数据页面](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\1.1.5.png)


### 2. 项目代码组件

> 基于数据抓取的流程，可以确定项目代码所需要的组件

- 消息中间件
- webapi
- node_server节点任务调度
- crawler爬虫

### 3. 消息中间件（消息总线）

> 使用redis作为消息总线

#### 3.1 token队列

- key：gsxt_token
- 功能：存储唯一识别码token，生产存储，等待被消费
- 数据结构：List

```
[token1, token2, ...]
```

#### 3.2 任务详情hashmap

- key：gsxt_task:token
- 功能：根据token存储各自抓取任务的详细信息，包含返回的数据
- 数据结构：Hashmap

```
{
	company_name, 
		str # 公司名称
	token, 
		str # 任务唯一识别码
	crack_captcha_mode, 
		str # 默认'0'(手动破解); '1'为调用打码平台破解
	status, 
		str # 'wait', 任务没开始
        str # 'crawling', 抓取中
    	str # 'failed', 失败
    	str # 'done', 完成
	msg, 
		str # status状态描述信息
	data, 
		json_str # 数据
	captcha_params,
		str # 图片验证码打码结果字符串
}
```


### 4. webapi

#### 4.1 功能

- 启动爬虫
- 手动打码
- 获取抓取结果

#### 4.2 设计接口

- 首页接口说明文档
- 爬虫启动接口
- 手动打码静态页面接口
- 手动打码获取验证码信息接口
- 获取任务状态或结果接口

#### 4.2.1 首页接口说明文档

```
GET /
```

#### 4.2.2 爬虫启动接口

```
GET /company
入参：
	company_name
	crack_captcha_mode
返回任务详情json_str：
	gsxt_task:token
```

#### 4.2.3 手动打码静态页面接口

```
GET /crack_captcha
入参：
	token
返回：
	手动打码的html页面
```

#### 4.2.4 手动打码获取验证码信息接口

```
POST /crack_captcha
入参：
	token
    captcha_params
返回任务详情json_str：
	gsxt_task:token
```

#### 4.2.5 获取任务状态或结果接口

```
GET /result
入参：
	token
返回任务详情json_str：
	gsxt_task:token
```

### 5. node_server节点任务调度

- 轮询gsxt_token队列，取出token
- 根据token从gsxt_task:token中读取任务信息
- 启动爬虫并传递参数

### 6. crawler爬虫

1. 根据入参抓取企业数据
2. 捕捉并记录异常的页面
3. 向gsxt_task:token中注册任务状态status和msg
4. 从gsxt_task:token中轮询读取打码结果captcha_params
5. 向gsxt_task:token返回数据data

### 7. 组件以及功能流程图

![gsxt-组件流程图](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\1.7.gsxt-组件流程图.png)

### 8. 项目运行流程

#### 8.1 接口启动爬虫

- 调用webapi启动爬虫/company接口生成token并向redis的gsxt_token队列中注册token，同时将任务详情字典写入gsxt_task:token

#### 8.2 节点调度启动爬虫

- node_server轮询gsxt_token队列，获取token，读取gsxt_task:token得到参数字典，启动crawler，并传入参数字典

#### 8.3 爬虫抓取

1. 初始化driver
2. 输入公司名称,并点击
3. 判断是否需要验证
4. 如果需要验证，获取验证图片并保存
5. 获取打码坐标
6. 点击验证图片
7. 判断查询结果
8. 选择第一条查询结果
9. 获取主要信息
10. 保存数据页面
11. 向redis中发送信息
12. 对失败情况进行保存,关闭driver,推送失败信息
13. 组织抓取逻辑，成功关闭driver

_________________

## 小结

1. 了解 获取数据的抓取流程
2. 了解 每个组件的功能
3. 了解 项目运行流程

## webapi代码实现

##### 学习目标：

1. 了解 quart框架
2. 了解 各个接口的功能

_________________

### 1. quart框架

#### 1.1 介绍

1. quart是基于Asyncio的Python微框架。它志在让开发者能够在Web开发中很容易地得到Asyncio带来的好处。它对Flask应用的支持最好，它和Flask拥有相同的API
2. 支持 HTTP/1.1，HTTP/2 和 Websockets
3. 扩展性很强，并支持很多Flask的扩展

#### 1.2 安装

`pip install quart`

#### 1.3 文档

> https://gitlab.com/pgjones/quart/

### 2. 代码实现

#### 2.1 webapi.py

> /gsxt/webapi.py

```
# THE WINTER IS COMING! the old driver will be driving who was a man of the world!
# -*- coding: utf-8 -*- python 3.6.7, create time is 18-11-9 下午7:27 GMT+8

import os
from uuid import uuid1
from base64 import b64encode
from redis import StrictRedis
from quart import (
    Quart,
    make_response,
    request,
    jsonify,
)

import static


redis = StrictRedis(decode_responses=True)
app = Quart(__name__)
app.config['JSON_AS_ASCII'] = False # 让jsonify()返回的json数据以utf8编码方式正常显示中文


@app.route('/', methods=['GET'])
async def index():
    """首页 接口说明"""
    headers = {"Content-type": "text/plain; charset=UTF-8"}
    resp = make_response((static.__doc__, 200, headers))
    return await resp

@app.route('/company', methods=['GET'])
async def company():
    """接收任务参数,写入redis"""
    company_name = request.args.get('company_name', None)
    if company_name is None:
        return jsonify({'status': 'failed', 'msg': '/company?company_name=缺少公司名称参数'})
    token = uuid1()
    # token = 'captcha_img' # 简单测试用
    gsxt_task_json = {
        'company_name': company_name,
        'crack_captcha_mode': request.args.get('crack_captcha_mode', None) or '0',
        'token': token,
        'status': 'wait',
        'msg': '新任务等待抓取'
    }
    try: # 向redis推任务
        task_key = '{}:{}'.format(static.GSXT_TASK_TOPIC, token)
        redis.hmset(task_key, gsxt_task_json)
        redis.expire(task_key, 600) # 过期时间
        redis.rpush(static.GSXT_TOKEN, token)
        redis.expire(static.GSXT_TOKEN, 590)
    except:
        gsxt_task_json['msg'] = '接收任务失败,检查redis及配置'
        return jsonify(gsxt_task_json)
    gsxt_task_json['msg'] = '成功接收抓取任务'
    return jsonify(gsxt_task_json)


@app.route('/crack_captcha', methods=['GET', 'POST'])
async def crack_captcha():
    """GET: 返回手动打码的html
    POST: 接收手动输入的验证码信息"""
    if request.method == 'GET':
        token = request.args.get('token', None)
        if token is None:
            return jsonify({'status': 'failed', 'msg': '/crack_captcha?token=缺少token参数'})
        img_file_path = './images/{}.jpg'.format(token) # 图片路径:以token命名图片
        # try:
        with open(img_file_path, 'rb') as f:
            img_b64_str = b64encode(f.read()).decode() # 为了前端渲染展示:图片二进制字节流转base64字符串
        # format(IMG_BASE64, CRACK_CAPTCHA_URL, TOKEN)
        return static.img_html%{'IMG_BASE64':img_b64_str,
                                'CRACK_CAPTCHA_URL':'/crack_captcha',
                                'TOKEN':token}
        # except:
        #     return jsonify({'status': 'failed', 'msg': '没有名为{}.jpg的图片,稍后重试或检查任务是否存在'.format(token)})

    elif request.method == 'POST':
        form_data = await request.form
        captcha_params = form_data.get('captcha_params', None)
        # print(captcha_params)
        token = form_data.get('token', None)
        # 将验证坐标存入redis
        task_key = '{}:{}'.format(static.GSXT_TASK_TOPIC, token)
        redis.hset(task_key, 'captcha_params', captcha_params)
        # os.remove('./images/{}.jpg'.format(token)) # 删除图片
        return jsonify({'token': token,
                        'result_url': '/result?token={}'.format(token),})


@app.route('/result')
async def result():
    """查询任务结果"""
    token = request.args.get('token', None)
    if token is None:
        return jsonify({'status': 'failed', 'msg': '/crack_captcha?token=缺少token参数'})
    task_key = '{}:{}'.format(static.GSXT_TASK_TOPIC, token)
    result_dict = redis.hgetall(task_key)
    return jsonify(result_dict)


if __name__ == '__main__':

    app.run()

```


#### 2.2 static.py

> /gsxt/static.py

```
"""
国家企业公示网实时数据抓取demo组件说明


webapi.py 交互演示功能
    任务参数push到redis消息总线中,
    数据查询也从redis消息总线中获取

    GET /
    接口说明文档

    GET /company
    接收任务参数
    params json = {
        company_name: 公司名称,
        crack_captcha_mode: 默认'0'(手动破解); '1'为调用打码平台破解,
    }
    return json = {
        company_name: 公司名称,
        token: 任务唯一识别码,
        crack_captcha_url: 手动破解验证的url,
    }

    GET /crack_captcha
    获取验证图片并返回手动破解验证码的html
    params json = {
        token: 任务唯一识别码,
    }
    return image in HTML

    POST /crack_captcha
    接收手动输入的验证码信息
    params json = {
        token: 任务唯一识别码,
        captcha_params: 验证码所需参数,
    }
    return json = {
        token: 任务唯一识别码,
        result_url: 查询抓取结果的url,
    }

    GET /result
    params json = {
        token: 任务唯一识别码,
    }

    return gsxt_task:token


redis 消息总线及缓存
    gsxt_token = [token1, token2, ...]

    gsxt_task:token
        {
            company_name,
            crack_captcha_mode,
            captcha_params,
            status, # 'wait', 任务没开始
                    # 'crawling', 抓取中
                    # 'failed', 失败
                    # 'done', 完成
            msg,
            data,
        }

crawler.py
    轮询redis
    抓取数据

static.py
    静态变量
    配置文件
"""

img_html = """<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>spider_gsxt</title>
</head>
<body>
    <div>
        <img id="imgPIC" src="data:image/jpg;base64,%(IMG_BASE64)s" alt="" onclick="Show(this)">
    </div>
    <div>
        X:<input id="xxx" type="text" />
        Y:<input id="yyy" type="text" />
        <h3>按顺序依次点击图中的字符,并点击提交</h3>
        <form action="%(CRACK_CAPTCHA_URL)s" method="post">
            <input type="text" id="captcha_params" name="captcha_params">
            <input type="hidden" value="%(TOKEN)s" name="token">
            <input type="submit" value="提交">
        </form>
    </div>
</body>
<script language="javascript">
    function mousePosition(ev){
        if(ev.pageX || ev.pageY){
            return {x:ev.pageX, y:ev.pageY};
          }
        return {
            x:ev.clientX + document.body.scrollLeft - document.body.clientLeft,
            y:ev.clientY + document.body.scrollTop  - document.body.clientTop
        };
    }
    function mouseMove(ev){
        ev = ev || window.event;
        var mousePos = mousePosition(ev);
        document.getElementById('xxx').value = mousePos.x;
        document.getElementById('yyy').value = mousePos.y;
    }
    document.onmousemove = mouseMove;
    function Show(el){
        var x = parseInt(document.getElementById('xxx').value)-el.offsetLeft;
        var y = parseInt(document.getElementById('yyy').value)-el.offsetTop;
        var captcha_params = document.getElementById('captcha_params').value;
        document.getElementById('captcha_params').value = captcha_params + x + "," + y + ",";
    }
</script>
<style>
    body{
        margin: 0;
        padding: 0;
    }
</style>
</html>"""

# 配置
GSXT_TOKEN = 'gsxt_token'
GSXT_TASK_TOPIC = 'gsxt_task'
```

_________________

## 小结

1. 了解 quart框架
2. 了解 各个接口的功能

## node_server节点任务调度实现

##### 学习目标：

1. 了解 节点任务调度模块的功能和实现

_________________


### 1. node_server节点任务调度的功能

- 轮询gsxt_token队列，取出token
- 根据token从gsxt_task:token中读取任务信息
- 启动爬虫并传递参数

### 2. node_server代码实现

> /gsxt/crawler.py

```
import time
import redis
import random
import requests
from PIL import Image # pip install pillow
from selenium import webdriver
from selenium.webdriver.common.action_chains import ActionChains

from static import (
    GSXT_TOKEN, # redis:list
    GSXT_TASK_TOPIC, # redis:hashmap
)

redis = redis.StrictRedis(decode_responses=True) # 多进程情况下需改为进程中的实例


class CrawlerServer():
    """任务接收并处理"""
    def crawl(self):
        """此处可以改为多任务并行"""
        while True:
            token = redis.lpop(GSXT_TOKEN)
            if token is not None:
                task_dict = redis.hgetall('{}:{}'.format(GSXT_TASK_TOPIC, token))
                print(task_dict)
                # input('用代理ip了吗?免费(推荐西刺代理,只能浏览器用)的比收费的好使!思考下为啥?继续吗?')
                # 调用爬虫
                spider = GsxtJSCrawler(task_dict)
                spider.run()
            print('等待任务中...')
            time.sleep(5)

......
```

### 3. node_server可拓展功能

#### 3.1 负载均衡 

> 通过`psutil os sys`等模块以及`shell`命令获取本节点的状态，根据设定的阈值（cpu、带宽、内存等）来决定是否抢夺redis中gsxt_token中的任务token

#### 3.2 服务注册与节点心跳

> 启动后向消息总线redis中实时注册节点的状态并发送心跳时间戳，用以检测节点的负载以及是否可用

#### 3.3 开启多任务执行crawler爬虫

> 以多进程的方式分别执行不同的token任务

_________________

## 小结

了解 节点任务调度模块的功能和实现

## crawler爬虫实现

##### 学习目标：

1. 了解 crawler爬虫运行流程
2. 了解 crawler爬虫模块实现

_________________

### 1. crawler功能

1. 初始化driver
2. 输入公司名称,并点击
3. 判断是否需要验证
4. 如果需要验证，获取验证图片并保存
5. 获取打码坐标
6. 点击验证图片
7. 判断查询结果
8. 选择第一条查询结果
9. 获取主要信息
10. 保存数据页面
11. 向redis中发送信息
12. 对失败情况进行保存,关闭driver,推送失败信息
13. 组织抓取逻辑，成功关闭driver

### 2. crawler代码实现

- 根据crawler的功能完成函数并组织运行逻辑

> /gsxt/crawler.py

```
......

class GsxtJSCrawler():
    """爬虫"""
    def __init__(self, task_dict={}):
        self.crack_captcha_mode = task_dict.get('crack_captcha_mode', '0') # 打码策略 '0'手动破解;'1'调用打码平台
        self.token = task_dict.get('token', None) # token
        self.company_name = task_dict.get('company_name', None) # 公司名称
        self.proxy = None # 代理ip
        # self.proxy = 'http://182.88.185.38:8123'

        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'}
        self.url = 'http://www.jsgsj.gov.cn:58888/province/' # 目前只有江苏 重庆等少数地区的接口还算稳定
        # self.url = 'http://www.gsxt.gov.cn/index.html'
        self.captcha_img = None # driver中的图片对象

        self.redis_key = '{}:{}'.format(GSXT_TASK_TOPIC, self.token)

        self.item = {} # 数据


    def init_driver(self):
        """初始化driver"""
        if self.proxy:
            opation = webdriver.ChromeOptions()
            opation.add_argument('--proxy-server={}'.format(self.proxy))
            self.driver = webdriver.Chrome('/home/worker/Desktop/driver/chromedriver', chrome_options=opation)
        else:
            self.driver = webdriver.Chrome('/home/worker/Desktop/driver/chromedriver')
            # self.driver = webdriver.PhantomJS('/home/worker/Desktop/driver/phantomjs')
        self.driver.get(self.url)
        time.sleep(2)
        self.driver.set_window_size(800, 600)


    def send_company_name(self):
        """输入公司名称,并点击"""
        self.driver.find_element_by_xpath('//*[@id="name"]').send_keys(self.company_name)
        time.sleep(1)
        self.driver.find_element_by_xpath('//a[@class="bt-chaxun"]').click()


    def check_captcha_img(self):
        """判断是否需要验证"""
        i = 0
        while i < 3:
            try: # 手动显式等待验证码图片出现
                time.sleep(1)
                # 获取图片对象
                self.captcha_img = self.driver.find_element_by_xpath('//img[@class="geetest_item_img"]')
                return # self.captcha_img != None
            except:
                # 存在不需要验证的情况; 也存在滑动的情况
                # 对于滑动拼图就pass
                if self.driver.current_url != self.url:
                    return # self.captcha_img = None
            i += 1


    def get_captcha_img(self):
        """获取验证图片并保存"""
        captcha_img_url = self.captcha_img.get_attribute('src')
        img_resp = requests.get(captcha_img_url, headers=self.headers)
        img = img_resp.content
        print(img)
        with open('./images/{}.jpg'.format(self.token), 'wb') as f:
            f.write(img)
        """width:100%, height:112%
        使用PIL模块"""
        im = Image.open('./images/{}.jpg'.format(self.token))
        width, height = im.size
        im.thumbnail((width, height / 1.12))
        im.save('./images/{}.jpg'.format(self.token), 'JPEG')


    def get_captcha_offset(self):
        """获取打码坐标"""
        # 手动打码
        if self.crack_captcha_mode == '0':
            i = 0
            while i < 180:
                captcha_offset = redis.hget(self.redis_key, 'captcha_params')
                if captcha_offset is not None:
                    return captcha_offset
                time.sleep(1)
                i += 1
            return None # 超时,没有获取打码坐标

        # 调用第三方打码
        elif self.crack_captcha_mode == '1':
            """暂不实现"""
            return None

        else:
            raise TypeError('仅支持webapi+redis+crawler组件模式的手动或者第三方打码方式')


    def click_captcha_offset(self, captcha_offset_str):
        """点击验证坐标"""
        captcha_offset = []
        # captcha_offset_str = '247,202,142,150,'
        captcha_offset_list = captcha_offset_str.split(',')[:-1]  # ['247', '202', '142', '150']
        for x in captcha_offset_list[::2]:
            y = captcha_offset_str.split(',')[:-1][captcha_offset_str.split(',')[:-1].index(x) + 1]
            captcha_offset.append((x, y))
        # captcha_offset = [('247', '202'), ('142', '150')]
        # captcha_offset = [(x, captcha_offset_str.split(',')[:-1][captcha_offset_str.split(',')[:-1].index(x)+1])
        #                   for x in captcha_offset_str.split(',')[:-1][::2]]
        """点击破解"""
        for i in range(len(captcha_offset)):
            ActionChains(self.driver).move_to_element_with_offset(
                to_element=self.captcha_img,
                xoffset=int(captcha_offset[i][0]) - 0, # 保存的图片和页面上图片大小不一致!
                yoffset=int(captcha_offset[i][1]) - 0).perform()
            # 时间要随机
            time.sleep(1)
            time.sleep(random.random())
            ActionChains(self.driver).click().perform()
        input('注意!这里不光需要模拟真人操作的随机,而且从出现验证图片开始就检测鼠标点击和轨迹!哪怕使用打码平台也要加入无用的鼠标动作!')
        # 点击确认提交
        self.driver.find_element_by_xpath('//a[@class="geetest_commit"]').click()
        time.sleep(2)
        # 判断点击是否成功
        captcha_img = self.driver.find_elements_by_xpath('//img[@class="geetest_item_img"]')
        return False if captcha_img != [] else True # 如果还有验证图片就说明失败了


    def check_result(self):
        """判断查询结果"""
        time.sleep(2)
        rets = self.driver.find_elements_by_xpath('//div[@class="listbox"]')
        return False if rets == [] else True


    def choice_first_result(self):
        """选择第一条查询结果"""
        time.sleep(2)
        self.driver.find_element_by_xpath('//div[@class="listbox"]/a[1]').click()
        """有时会跳出新的标签页,所以根据句柄强行切换到最后一个标签页"""
        self.driver.switch_to.window(self.driver.window_handles[-1])


    def get_baseinfo_item(self):
        """获取主要信息
        """
        i = 0
        while i<5: # 手动显式等待,等待页面加载完毕,以reg_no是否出现为标志
            reg_no = self.driver.find_elements_by_xpath('//*[@id="REG_NO"]')
            if reg_no != []:
                break
            time.sleep(3)
            i += 1
        # 统一社会信用代码/注册号REG_NO
        reg_no = self.driver.find_elements_by_xpath('//*[@id="REG_NO"]')
        if reg_no == []:
            return False
        self.item['reg_no'] = reg_no[0].text if id != [] else ''
        # 企业名称CORP_NAME
        corp_name = self.driver.find_elements_by_xpath('//*[@id="CORP_NAME"]')
        self.item['corp_name'] = corp_name[0].text if id != [] else ''
        # 类型ZJ_ECON_KIND
        zj_econ_kind = self.driver.find_elements_by_xpath('//*[@id="ZJ_ECON_KIND"]')
        self.item['zj_econ_kind'] = zj_econ_kind[0].text if id != [] else ''
        # 法定代表人OPER_MAN_NAME
        oper_man_name = self.driver.find_elements_by_xpath('//*[@id="OPER_MAN_NAME"]')
        self.item['oper_man_name'] = oper_man_name[0].text if id != [] else ''
        # 注册资本REG_CAPI
        reg_cpi = self.driver.find_elements_by_xpath('//*[@id="REG_CAPI"]')
        self.item['reg_cpi'] = reg_cpi[0].text if id != [] else ''
        # 成立日期START_DATE
        start_date = self.driver.find_elements_by_xpath('//*[@id="START_DATE"]')
        self.item['oper_man_name'] = start_date[0].text if id != [] else ''
        # 营业期限自FARE_TERM_START
        fare_term_start = self.driver.find_elements_by_xpath('//*[@id="FARE_TERM_START"]')
        self.item['fare_term_start'] = fare_term_start[0].text if id != [] else ''
        # 营业期限至FARE_TERM_END
        fare_term_end = self.driver.find_elements_by_xpath('//*[@id="FARE_TERM_END"]')
        self.item['fare_term_end'] = fare_term_end[0].text if id != [] else ''
        # 登记机关BELONG_ORG
        belong_org = self.driver.find_elements_by_xpath('//*[@id="BELONG_ORG"]')
        self.item['belong_org'] = belong_org[0].text if id != [] else ''
        # 核准日期CHECK_DATE
        check_date = self.driver.find_elements_by_xpath('//*[@id="CHECK_DATE"]')
        self.item['check_date'] = check_date[0].text if id != [] else ''
        # 登记状态CORP_STATUS
        corp_status = self.driver.find_elements_by_xpath('//*[@id="CORP_STATUS"]')
        self.item['corp_status'] = corp_status[0].text if id != [] else ''
        # 住所ADDR
        addr = self.driver.find_elements_by_xpath('//*[@id="ADDR"]')
        self.item['addr'] = addr[0].text if id != [] else ''
        # 经营范围FARE_SCOPE
        fare_scope = self.driver.find_elements_by_xpath('//*[@id="FARE_SCOPE"]')
        self.item['fare_scope'] = fare_scope[0].text if id != [] else ''
        return True


    def save_html(self):
        """保存首页数据页面,后续可提取完整信息
        同样可以保存其他数据页
        """
        file_name = './html/{}_base.html'.format(self.item['reg_no'])
        with open(file_name, 'w') as f:
            f.write(self.driver.page_source)


    def save_fail(self, msg):
        """保存失败情况,关闭driver,推送失败信息"""
        # self.driver.save_screenshot('./error/{}.png'.format(self.token)) # 70版本的chrome不能调用截图功能
        print(msg)
        file_name = './error/{}_base.html'.format(self.token)
        with open(file_name, 'w') as f:
            f.write(self.driver.page_source)
        self.driver.quit() # 先保存失败,再关闭driver!
        self.send_msg_to_redis(msg=msg, status='failed')


    def send_msg_to_redis(self, msg, status):
        """向redis中发送信息"""
        redis.hset(self.redis_key, 'status', status)
        redis.hset(self.redis_key, 'msg', msg)


    def _main(self):
        """抓取逻辑"""
        if self.company_name is None:
            print('没有公司名称,查个毛线')
            return
        if self.token is None:
            print('想单文件抓取自己写啊!')
            return
        try: # 初始化driver
            self.init_driver()
        except:
            self.save_fail('初始化失败')
            return

        self.send_msg_to_redis(msg='抓取进行中', status='crawling')

        try: # 输入公司名称点击
            self.send_company_name()
        except:
            input(11)
            self.save_fail('输入公司名称点击失败')
            return

        self.check_captcha_img() # 检查是否需要验证
        if self.captcha_img is not None: # 需要验证的逻辑
            self.get_captcha_img() # 获取验证图片并保存
            captcha_offset_str = self.get_captcha_offset() # 获取打码结果
            print(captcha_offset_str)
            ret = self.click_captcha_offset(captcha_offset_str) # 点击验证坐标
            if not ret: # 验证点击失败
                self.save_fail('验证点击失败, 点对了也失败是因为同一ip访问次数过多, 请更换代理ip')
                return
        if not self.check_result(): # 判断 没有结果就结束
            self.save_fail('查询失败')
            return

        """仅对结果列表中第一个搞事情
        拿到所有html的page_source,并只返回主要信息
        提取数据的思路:提取一点就保存一点!
        """
        self.choice_first_result() # 选择结果列表中第一个
        try:
            self.get_baseinfo_item() # 主要信息
            print(self.item)
        except:
            self.save_fail('提取数据失败')

        self.save_html()  # 保存数据页面,后续可以提取完整信息
        # 先推数据,后推消息
        redis.hset(self.redis_key, 'data', self.item) # 向redis存数据
        self.send_msg_to_redis(msg='抓取成功', status='done')

        self.driver.quit() # 关闭浏览器
        # self.driver.service.process.pid # webdriver-server的pid


    def run(self):
        self._main()


if __name__ == '__main__':

    server = CrawlerServer()
    server.crawl()


```

### 3. 完成后的项目文件结构

![项目文件结构](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\4.3.png)

### 4. 后续可以继续完善

- 抓取更多的字段
- 保存更多的数据页面
- 以token命名，记录详细的日志信息
- 对接第三方打码平台

_________________

## 小结

1. 了解 crawler爬虫运行流程
2. 了解 crawler爬虫模块实现

## 运行效果

##### 学习目标：

无

_________________


### 0. 首页--接口/说明文档

![首页](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\5.0.png)

### 1. 启动爬虫

> 127.0.0.1:5000/company?company_name=

![启动爬虫返回页](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\5.1.png)

### 2. 访问手动打码页面

> 127.0.0.1:5000/crack_captcha?token= 

![手动打码页](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\5.2.png)

### 3. 打码后返回页

![打码后返回页](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\5.3.png)

### 4. 查询结果

> 127.0.0.1:5000/result?token=

#### 4.1 抓取中

![抓取中](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\5.4.png)

#### 4.2 抓取成功

![抓取成功](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\5.5.png)

#### 4.3 抓取失败

![抓取失败](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\5.6.png)

### 5. 静态文件夹

![静态文件夹](D:\hgx笔记\hgxbijiben\4、爬虫知识\课件\10-项目-国家企业公示网\images\5.7.png)

- error：保存异常页和截图
- html：保存数据页
- images：保存打码图片

_________________

## 小结

无



